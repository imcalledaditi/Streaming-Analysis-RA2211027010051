# model/infer.py

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_PATH = "models/best_model.pt"
MODEL_NAME = "distilbert-base-uncased"
MAX_LENGTH = 128
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Emotion labels from GoEmotions
EMOTION_LABELS = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',
    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',
    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness',
    'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise',
    'neutral'
]

# Tokenizer and Model Setup
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(EMOTION_LABELS),
    id2label={i: label for i, label in enumerate(EMOTION_LABELS)},
    label2id={label: i for i, label in enumerate(EMOTION_LABELS)}
)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.to(DEVICE)
model.eval()

def predict_emotion(text: str) -> tuple:
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length"
    )
    input_ids = inputs["input_ids"].to(DEVICE)
    attention_mask = inputs["attention_mask"].to(DEVICE)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_class_id = torch.argmax(logits, dim=1).item()

    emotion = EMOTION_LABELS[predicted_class_id]

    # Simple heuristic for satisfaction
    satisfaction_map = {
        "joy": "satisfied",
        "gratitude": "satisfied",
        "relief": "satisfied",
        "love": "satisfied",
        "admiration": "satisfied",
        "approval": "satisfied",
        "neutral": "neutral",
    }
    satisfaction = satisfaction_map.get(emotion, "unsatisfied")

    return emotion, satisfaction

